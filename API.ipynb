{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-bjHa0BZzG6",
        "outputId": "82d2a26d-b95a-4b1f-bd71-23c35e5922f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfHeM2yUMfic",
        "outputId": "e542d608-8b25-4663-9ed3-61e45acbdd7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.39-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.39-py3-none-any.whl (896 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m896.9/896.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.12-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.39 ultralytics-thop-2.0.12\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afkGjqlCSbn0",
        "outputId": "51b5a34f-e01b-4c1a-90b3-6afdedc6befc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.8.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.5)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Downloading mediapipe-0.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.1/36.1 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.18 sounddevice-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Euth2PDLJfK",
        "outputId": "7ecce8db-159f-45d9-dd36-a7d377ccfe1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Importing the drive module from Google Colab to mount Google Drive (Authorization).\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB5kgWG5Kq0e",
        "outputId": "e1235b5c-c0e4-4e7f-a5c1-4ab910887bf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, render_template, jsonify  # Import Flask and other modules for web application\n",
        "from flask import Flask, send_file  # Import Flask and send_file for serving images\n",
        "from io import BytesIO  # Import BytesIO for handling binary data\n",
        "import matplotlib.pyplot as plt  # Import matplotlib for plotting images\n",
        "from PIL import Image  # Import PIL for image processing\n",
        "import matplotlib.image as mpimg  # Import matplotlib.image for reading images\n",
        "from ultralytics import YOLO # YOLO object detection model\n",
        "import mediapipe as mp # Mediapipe for pose estimation\n",
        "import cv2 # OpenCV for image processing\n",
        "import os # For file operations\n",
        "import random # For random number generation\n",
        "from matplotlib.patches import Rectangle  # For drawing rectangles on images\n",
        "\n",
        "import math  # For mathematical operations\n",
        "import io  # For handling binary data\n",
        "import base64  # For encoding/decoding binary data\n",
        "import numpy as np  # For numerical operations\n",
        "from operator import imod  # For modulo operation\n",
        "\n",
        "from pyngrok import ngrok  # For exposing local server to the internet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t7tXD5_Kq0j"
      },
      "outputs": [],
      "source": [
        "# Load a trained model\n",
        "model = YOLO('/content/drive/MyDrive/Hassana/Run/runs/detect/train/weights/best.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRtlarzfKq0k",
        "outputId": "fe4935ec-094c-4d57-d48b-c79c0c04eae9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://ad23-34-169-97-50.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /content/1000000074.jpg: 448x640 1 raising, 64.8ms\n",
            "Speed: 16.6ms preprocess, 64.8ms inference, 1172.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Bounding box coordinates (x1, y1, x2, y2): 805.905517578125 223.730224609375 1770.93115234375 1696.0\n",
            "The predicted class is (805, 223, 1770, 1696)\n",
            "\n",
            "image 1/1 /content/1000000074.jpg: 448x640 1 raising, 8.7ms\n",
            "Speed: 3.8ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "40.42092565025\n",
            "Done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [02/Dec/2024 08:00:02] \"POST /Ruku HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /content/1000000074.jpg: 448x640 1 raising, 35.3ms\n",
            "Speed: 2.4ms preprocess, 35.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Bounding box coordinates (x1, y1, x2, y2): 805.905517578125 223.730224609375 1770.93115234375 1696.0\n",
            "The predicted class is (805, 223, 1770, 1696)\n",
            "\n",
            "image 1/1 /content/1000000074.jpg: 448x640 1 raising, 9.1ms\n",
            "Speed: 3.7ms preprocess, 9.1ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([0.], device='cuda:0')\n",
            "conf: tensor([0.9424], device='cuda:0')\n",
            "data: tensor([[8.0591e+02, 2.2373e+02, 1.7709e+03, 1.6960e+03, 9.4242e-01, 0.0000e+00]], device='cuda:0')\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (1696, 2560)\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[1288.4183,  959.8651,  965.0256, 1472.2698]], device='cuda:0')\n",
            "xywhn: tensor([[0.5033, 0.5660, 0.3770, 0.8681]], device='cuda:0')\n",
            "xyxy: tensor([[ 805.9055,  223.7302, 1770.9312, 1696.0000]], device='cuda:0')\n",
            "xyxyn: tensor([[0.3148, 0.1319, 0.6918, 1.0000]], device='cuda:0')\n",
            "Bounding box coordinates (x1, y1, x2, y2): 805.905517578125 223.730224609375 1770.93115234375 1696.0\n",
            "2.006045627593994\n",
            "0.07530397176742554\n",
            "0.004847168922424316\n",
            "2.006045627593994\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [02/Dec/2024 08:00:36] \"POST /Raising HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /content/1000000089.jpg: 640x480 1 ruku, 45.2ms\n",
            "Speed: 3.3ms preprocess, 45.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Bounding box coordinates (x1, y1, x2, y2): 413.9988098144531 1041.233642578125 2096.080078125 3075.3828125\n",
            "The predicted class is (413, 1041, 2096, 3075)\n",
            "\n",
            "image 1/1 /content/1000000089.jpg: 640x480 1 ruku, 10.6ms\n",
            "Speed: 4.6ms preprocess, 10.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "5.5854333028576235\n",
            "Done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [02/Dec/2024 08:09:25] \"POST /Ruku HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /content/1000000089.jpg: 640x480 1 ruku, 34.8ms\n",
            "Speed: 2.8ms preprocess, 34.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Bounding box coordinates (x1, y1, x2, y2): 413.9988098144531 1041.233642578125 2096.080078125 3075.3828125\n",
            "The predicted class is (413, 1041, 2096, 3075)\n",
            "\n",
            "image 1/1 /content/1000000089.jpg: 640x480 1 ruku, 9.8ms\n",
            "Speed: 4.5ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([1.], device='cuda:0')\n",
            "conf: tensor([0.8187], device='cuda:0')\n",
            "data: tensor([[4.1400e+02, 1.0412e+03, 2.0961e+03, 3.0754e+03, 8.1874e-01, 1.0000e+00]], device='cuda:0')\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (3264, 2448)\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[1255.0394, 2058.3081, 1682.0813, 2034.1492]], device='cuda:0')\n",
            "xywhn: tensor([[0.5127, 0.6306, 0.6871, 0.6232]], device='cuda:0')\n",
            "xyxy: tensor([[ 413.9988, 1041.2336, 2096.0801, 3075.3828]], device='cuda:0')\n",
            "xyxyn: tensor([[0.1691, 0.3190, 0.8562, 0.9422]], device='cuda:0')\n",
            "Bounding box coordinates (x1, y1, x2, y2): 413.9988098144531 1041.233642578125 2096.080078125 3075.3828125\n",
            "0.05594708702780984\n",
            "0.049233436584472656\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [02/Dec/2024 08:09:53] \"POST /Takbeer HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /content/1000000089.jpg: 640x480 1 ruku, 55.1ms\n",
            "Speed: 4.2ms preprocess, 55.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Bounding box coordinates (x1, y1, x2, y2): 413.9988098144531 1041.233642578125 2096.080078125 3075.3828125\n",
            "The predicted class is (413, 1041, 2096, 3075)\n",
            "\n",
            "image 1/1 /content/1000000089.jpg: 640x480 1 ruku, 13.3ms\n",
            "Speed: 9.6ms preprocess, 13.3ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "5.5854333028576235\n",
            "Done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [02/Dec/2024 08:10:37] \"POST /Ruku HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "app = Flask(__name__)\n",
        "ngrok.set_auth_token(\"2oq6itLuYMRRLskGyq4t7jnTwkw_3FGnQttFhNCNYQuzCrTFb\")\n",
        "public_url =  ngrok.connect(5000).public_url\n",
        "print(public_url)\n",
        "\n",
        "# Load a trained model\n",
        "model = YOLO('/content/drive/MyDrive/Hassana/Run/runs/detect/train/weights/best.pt')\n",
        "\n",
        "@app.route('/Ruku', methods=['POST', 'GET'])\n",
        "def Detect_Ruku():\n",
        "    imagefile = request.files['imagefile']\n",
        "    image_path = imagefile.filename\n",
        "    imagefile.save(image_path)\n",
        "\n",
        "    results = model.predict(image_path)\n",
        "\n",
        "\n",
        "    for r in results:\n",
        "        # Extract bounding box coordinates\n",
        "        bounding_box = x1, y1, x2, y2 = r.boxes.xyxy[0].tolist()\n",
        "        print(\"Bounding box coordinates (x1, y1, x2, y2):\", x1, y1, x2, y2)\n",
        "        bounding_box_tuple = tuple(bounding_box)\n",
        "\n",
        "\n",
        "    bounding_box_int = tuple(int(value) for value in bounding_box_tuple)\n",
        "\n",
        "    print('The predicted class is ' + str(bounding_box_int))\n",
        "\n",
        "\n",
        "    # Initialize Mediapipe Pose model\n",
        "    mp_pose = mp.solutions.pose\n",
        "    pose = mp_pose.Pose()\n",
        "\n",
        "\n",
        "    # Function to calculate angle between lines formed by three points\n",
        "    def calculate_angle(a, b, c):\n",
        "        angle_rad1 = math.atan2(c[1]-b[1], c[0]-b[0]) - math.atan2(a[1]-b[1], a[0]-b[0])\n",
        "        angle_rad1 = angle_rad1 % (2 * math.pi)\n",
        "        angle_rad2 = math.atan2(a[1]-b[1], a[0]-b[0]) - math.atan2(c[1]-b[1], c[0]-b[0])\n",
        "        angle_rad2 = angle_rad2 % (2 * math.pi)\n",
        "        return math.degrees(min(angle_rad1, angle_rad2))\n",
        "\n",
        "    # Function to check if the pose resembles the ruku position in Salat\n",
        "    def is_ruku_pose(landmarks):\n",
        "        left_hip = (landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y)\n",
        "        right_hip = (landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y)\n",
        "        left_shoulder = (landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y)\n",
        "        right_shoulder = (landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y)\n",
        "        left_knee = (landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y)\n",
        "        right_knee = (landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y)\n",
        "\n",
        "        angle_left = abs(90 - calculate_angle(left_knee, left_hip, left_shoulder))\n",
        "        angle_right = abs(90 - calculate_angle(right_knee, right_hip, right_shoulder))\n",
        "\n",
        "        ruku_angle_threshold = 30\n",
        "        if angle_left <= ruku_angle_threshold or angle_right <= ruku_angle_threshold:\n",
        "            return True, min(abs(angle_left), abs(angle_right)) / 100 * 50 # Calculate error percentage\n",
        "        else:\n",
        "            return False, min(abs(angle_left), abs(angle_right)) / 100 * 50\n",
        "\n",
        "    def process_image(image_path):\n",
        "        results = model.predict(image_path)\n",
        "\n",
        "        for r in results:\n",
        "            bounding_box = x1, y1, x2, y2 = r.boxes.xyxy[0].tolist()\n",
        "\n",
        "        bounding_box = tuple(int(value) for value in bounding_box)\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        results = pose.process(image_rgb)\n",
        "        is_ruku, error_percentage = is_ruku_pose(results.pose_landmarks.landmark)\n",
        "        print(error_percentage)\n",
        "\n",
        "        # Red color if error>15, else green\n",
        "\n",
        "        if error_percentage<= 15:\n",
        "          color = (0, 255, 0)\n",
        "        else:\n",
        "          color = (255, 0, 0)\n",
        "\n",
        "        # Line thickness of 3 px\n",
        "        thickness = 3\n",
        "\n",
        "            # Draw bounding box\n",
        "        x1, y1, x2, y2 = bounding_box\n",
        "        cv2.rectangle(image_rgb, (x1, y1), (x2, y2), color, thickness)\n",
        "\n",
        "\n",
        "        Perfect_Threshold = 5;\n",
        "\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(image_rgb)\n",
        "        mp_drawing = mp.solutions.drawing_utils\n",
        "        mp_drawing.draw_landmarks(image_rgb, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
        "#onvert these normalized coordinates to pixel coordinates, we need to scale them according to the size of the actual image in pixels\n",
        "        for landmark in [mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.RIGHT_KNEE,\n",
        "                        mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP,\n",
        "                        mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER]:\n",
        "            plt.scatter(results.pose_landmarks.landmark[landmark.value].x * image.shape[1],\n",
        "                        results.pose_landmarks.landmark[landmark.value].y * image.shape[0],\n",
        "                        color='white', s=100)\n",
        "\n",
        "        plt.plot([results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP.value].x * image.shape[1],\n",
        "                results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_KNEE.value].x * image.shape[1]],\n",
        "                [results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP.value].y * image.shape[0],\n",
        "                results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_KNEE.value].y * image.shape[0]],\n",
        "                color='cyan', linestyle='-', linewidth=2, alpha=0.5)\n",
        "        plt.plot([results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP.value].x * image.shape[1],\n",
        "                results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE.value].x * image.shape[1]],\n",
        "                [results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP.value].y * image.shape[0],\n",
        "                results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE.value].y * image.shape[0]],\n",
        "                color='cyan', linestyle='-', linewidth=2, alpha=0.5)\n",
        "\n",
        "        plt.plot([results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP.value].x * image.shape[1],\n",
        "                results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x * image.shape[1]],\n",
        "                [results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP.value].y * image.shape[0],\n",
        "                results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y * image.shape[0]],\n",
        "                color='magenta', linestyle='-', linewidth=2, alpha=0.5)\n",
        "        plt.plot([results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP.value].x * image.shape[1],\n",
        "                results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x * image.shape[1]],\n",
        "                [results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP.value].y * image.shape[0],\n",
        "                results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y * image.shape[0]],\n",
        "                color='magenta', linestyle='-', linewidth=3, alpha=0.5)\n",
        "\n",
        "        print('Done')\n",
        "        plt.axis('off')\n",
        "        buffer = io.BytesIO()\n",
        "        plt.savefig(buffer, format='png')\n",
        "        buffer.seek(0)\n",
        "        image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
        "        plt.close()\n",
        "        return image_base64, error_percentage, is_ruku\n",
        "\n",
        "\n",
        "\n",
        "    # Process the image and get the base64 encoded plot\n",
        "    plot_base64, error, is_ruku= process_image(image_path)\n",
        "    error_text = \"{:.2f}\".format(error)\n",
        "\n",
        "    if is_ruku and error <= 20:\n",
        "        return jsonify({'image_base64': plot_base64, 'is_pose': True})\n",
        "    else:\n",
        "        return jsonify({'image_base64': plot_base64, 'is_pose': False})\n",
        "\n",
        "\n",
        "@app.route('/Takbeer', methods=['POST', 'GET'])\n",
        "def Detect_Takbeer():\n",
        "    imagefile = request.files['imagefile']\n",
        "    image_path =  imagefile.filename\n",
        "    imagefile.save(image_path)\n",
        "\n",
        "    results = model.predict(image_path)\n",
        "\n",
        "\n",
        "    for r in results:\n",
        "        # Extract bounding box coordinates\n",
        "        bounding_box = x1, y1, x2, y2 = r.boxes.xyxy[0].tolist()\n",
        "        print(\"Bounding box coordinates (x1, y1, x2, y2):\", x1, y1, x2, y2)\n",
        "        bounding_box_tuple = tuple(bounding_box)\n",
        "\n",
        "\n",
        "    bounding_box_int = tuple(int(value) for value in bounding_box_tuple)\n",
        "\n",
        "    print('The predicted class is ' + str(bounding_box_int))\n",
        "\n",
        "\n",
        "    # Initialize Mediapipe Pose model\n",
        "    mp_pose = mp.solutions.pose\n",
        "    pose = mp_pose.Pose()\n",
        "\n",
        "    # Function to calculate angle between lines formed by three points\n",
        "    def calculate_angle(a, b, c):\n",
        "        angle_rad = math.atan2(c[1]-b[1], c[0]-b[0]) - math.atan2(a[1]-b[1], a[0]-b[0])\n",
        "        angle_rad = angle_rad % (2 * math.pi)\n",
        "        return math.degrees(angle_rad)\n",
        "\n",
        "    # Function to check if the pose resembles the Takbeer position in Salat\n",
        "    def is_takbeer_pose(landmarks):\n",
        "        left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
        "        right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
        "        left_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
        "        right_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\n",
        "        left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
        "        right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
        "        left_index = landmarks[mp_pose.PoseLandmark.LEFT_INDEX.value]\n",
        "        right_index = landmarks[mp_pose.PoseLandmark.RIGHT_INDEX.value]\n",
        "\n",
        "        print(abs(abs(left_wrist.y)-abs(right_wrist.y)) / 0.22 * 100)\n",
        "\n",
        "        Perfect_Threshold = 5;\n",
        "        # Check if left and right wrist are on the same line\n",
        "        if abs(left_wrist.y - right_wrist.y) < 0.05 and abs(left_wrist.x - right_wrist.x) > 0.15:\n",
        "            # Check if distance between right hip and right shoulder in y-direction is big enough\n",
        "            if right_hip.y - right_shoulder.y > 0.25 or left_hip.y - left_shoulder.y > 0.25 :\n",
        "              return True, abs(abs(left_wrist.y)-abs(right_wrist.y)) / 0.25 * 100\n",
        "        return False, abs(abs(left_wrist.y)-abs(right_wrist.y)) / 0.25 * 100\n",
        "\n",
        "    def process_image(image_path):\n",
        "        model_results = model.predict(image_path)\n",
        "\n",
        "        for r in model_results:\n",
        "            print(r.boxes)\n",
        "            # Extract bounding box coordinates\n",
        "            bounding_box = x1, y1, x2, y2 = r.boxes.xyxy[0].tolist()\n",
        "            print(\"Bounding box coordinates (x1, y1, x2, y2):\", x1, y1, x2, y2)\n",
        "            bounding_box_tuple = tuple(bounding_box)\n",
        "\n",
        "        bounding_box = tuple(int(value) for value in bounding_box_tuple)\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        # Detect pose landmarks\n",
        "        results = pose.process(image_rgb)\n",
        "        is_takbeer, error = is_takbeer_pose(results.pose_landmarks.landmark)\n",
        "        print(error)\n",
        "\n",
        "        # Red color if error>15, else green\n",
        "\n",
        "        if error<= 15 and is_takbeer:\n",
        "          color = (0, 255, 0)\n",
        "        else:\n",
        "          color = (255, 0, 0)\n",
        "\n",
        "        # Line thickness of 3 px\n",
        "        thickness = 3\n",
        "\n",
        "\n",
        "        # Load image\n",
        "        # Draw bounding box\n",
        "        x1, y1, x2, y2 = bounding_box\n",
        "        cv2.rectangle(image_rgb, (x1, y1), (x2, y2), color, thickness)\n",
        "\n",
        "        # Convert image to RGB\n",
        "\n",
        "        Perfect_Threshold = 5;\n",
        "\n",
        "\n",
        "\n",
        "        # Draw text above bounding box with background color\n",
        "        # Draw text above bounding box with background color\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(image_rgb)\n",
        "\n",
        "        # Draw connections between all specified landmarks\n",
        "        landmarks = results.pose_landmarks.landmark\n",
        "        connections = [(mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER),\n",
        "                      (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_HIP),\n",
        "                      (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_HIP),\n",
        "                      (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP),\n",
        "                      (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_WRIST),\n",
        "                      (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_WRIST),\n",
        "                      (mp_pose.PoseLandmark.LEFT_WRIST, mp_pose.PoseLandmark.LEFT_INDEX),\n",
        "                      (mp_pose.PoseLandmark.RIGHT_WRIST, mp_pose.PoseLandmark.RIGHT_INDEX)]\n",
        "\n",
        "        # Draw points for knees, hips, and shoulders\n",
        "        for landmark in [mp_pose.PoseLandmark.LEFT_INDEX, mp_pose.PoseLandmark.RIGHT_INDEX,\n",
        "                        mp_pose.PoseLandmark.LEFT_WRIST, mp_pose.PoseLandmark.RIGHT_WRIST,\n",
        "                            mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP,\n",
        "                            mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER]:\n",
        "                plt.scatter(results.pose_landmarks.landmark[landmark.value].x * image.shape[1],\n",
        "                            results.pose_landmarks.landmark[landmark.value].y * image.shape[0],\n",
        "                            color='white', s=100)\n",
        "\n",
        "        for connection in connections:\n",
        "            start_point = connection[0]\n",
        "            end_point = connection[1]\n",
        "            plt.plot([landmarks[start_point.value].x * image.shape[1], landmarks[end_point.value].x * image.shape[1]],\n",
        "                    [landmarks[start_point.value].y * image.shape[0], landmarks[end_point.value].y * image.shape[0]],\n",
        "                    color='magenta', linestyle='-', linewidth=1)\n",
        "        # Draw landmarks and connections\n",
        "        mp_drawing = mp.solutions.drawing_utils\n",
        "        mp_drawing.draw_landmarks(image_rgb, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
        "                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=5),\n",
        "                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2))\n",
        "        plt.axis('off')\n",
        "        # Save the plot as bytes in memory\n",
        "        buffer = io.BytesIO()\n",
        "        plt.savefig(buffer, format='png')\n",
        "        buffer.seek(0)\n",
        "        # Convert the plot bytes to base64 string\n",
        "        plot_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
        "        plt.close()  # Close the plot to free memory\n",
        "\n",
        "        return plot_base64, error, is_takbeer\n",
        "\n",
        "    # Process the image and get the base64 encoded plot\n",
        "    plot_base64,error, is_takbeer = process_image(image_path)\n",
        "\n",
        "    error_text = \"{:.2f}\".format(error)\n",
        "\n",
        "\n",
        "    if is_takbeer and error <= 20:\n",
        "        return jsonify({'image_base64': plot_base64, 'is_pose': True})\n",
        "    else:\n",
        "        return jsonify({'image_base64': plot_base64, 'is_pose': False})\n",
        "\n",
        "\n",
        "\n",
        "@app.route('/Raising', methods=['POST', 'GET'])\n",
        "def Detect_Raising():\n",
        "    imagefile = request.files['imagefile']\n",
        "    image_path = imagefile.filename\n",
        "    imagefile.save(image_path)\n",
        "\n",
        "    results = model.predict(image_path)\n",
        "\n",
        "\n",
        "    for r in results:\n",
        "        # Extract bounding box coordinates\n",
        "        bounding_box = x1, y1, x2, y2 = r.boxes.xyxy[0].tolist()\n",
        "        print(\"Bounding box coordinates (x1, y1, x2, y2):\", x1, y1, x2, y2)\n",
        "        bounding_box_tuple = tuple(bounding_box)\n",
        "\n",
        "\n",
        "    bounding_box_int = tuple(int(value) for value in bounding_box_tuple)\n",
        "\n",
        "    print('The predicted class is ' + str(bounding_box_int))\n",
        "\n",
        "\n",
        "    # Initialize Mediapipe Pose model\n",
        "    mp_pose = mp.solutions.pose\n",
        "    pose = mp_pose.Pose()\n",
        "\n",
        "    def is_raising_pose(landmarks):\n",
        "        # Extract relevant landmarks\n",
        "        left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
        "        right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
        "        left_elbow = landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value]\n",
        "        right_elbow = landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value]\n",
        "        left_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
        "        right_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\n",
        "        nose = landmarks[mp_pose.PoseLandmark.NOSE.value]\n",
        "        chin = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
        "        left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
        "        right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
        "\n",
        "        print(abs(abs(abs(left_wrist.y)-abs(right_wrist.y))+abs(abs(left_wrist.x)-abs(right_wrist.x))-0.03)/ 2.5 * 100)\n",
        "        print(left_wrist.x - right_wrist.x)\n",
        "        print(left_wrist.y - right_wrist.y)\n",
        "\n",
        "        # Check if left and right wrist are close to each other\n",
        "        if abs(left_wrist.x - right_wrist.x) and abs(left_wrist.y - right_wrist.y) < 0.05:\n",
        "            # Check if distance between right hip and right shoulder in y-direction is big enough\n",
        "            if right_hip.y - right_shoulder.y > 0.25 or left_hip.y - left_shoulder.y > 0.25 :\n",
        "                if right_wrist.y - right_shoulder.y > 0.15 or left_wrist.y - left_shoulder.y > 0.15:\n",
        "                    # Check  wrists are above the shoulders\n",
        "                    return True, abs(abs(abs(left_wrist.y)-abs(right_wrist.y))+abs(abs(left_wrist.x)-abs(right_wrist.x))-0.03)/ 2.5 * 100\n",
        "        return False, abs(abs(abs(left_wrist.y)-abs(right_wrist.y))+abs(abs(left_wrist.x)-abs(right_wrist.x))-0.03)/ 2.5 * 100\n",
        "\n",
        "    def process_image(image_path):\n",
        "        # Load Model\n",
        "        results = model.predict(image_path)\n",
        "\n",
        "        for r in results:\n",
        "            print(r.boxes)\n",
        "            # Extract bounding box coordinates\n",
        "            bounding_box = x1, y1, x2, y2 = r.boxes.xyxy[0].tolist()\n",
        "            print(\"Bounding box coordinates (x1, y1, x2, y2):\", x1, y1, x2, y2)\n",
        "            bounding_box_tuple = tuple(bounding_box)\n",
        "\n",
        "        bounding_box = tuple(int(value) for value in bounding_box_tuple)\n",
        "\n",
        "        # Load image\n",
        "        image = cv2.imread(image_path)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        # Detect pose landmarks\n",
        "        results = pose.process(image_rgb)\n",
        "        is_raising, error = is_raising_pose(results.pose_landmarks.landmark)\n",
        "        print(error)\n",
        "\n",
        "\n",
        "\n",
        "        # Red color if error>15, else green\n",
        "\n",
        "        if error<= 15 and is_raising:\n",
        "          color = (0, 255, 0)\n",
        "        else:\n",
        "          color = (255, 0, 0)\n",
        "\n",
        "        # Line thickness of 3 px\n",
        "        thickness = 3\n",
        "\n",
        "\n",
        "        # Draw bounding box\n",
        "        x1, y1, x2, y2 = bounding_box\n",
        "        cv2.rectangle(image_rgb, (x1, y1), (x2, y2), color, thickness)\n",
        "\n",
        "        # Convert image to RGB\n",
        "\n",
        "        Perfect_Threshold = 5;\n",
        "\n",
        "\n",
        "\n",
        "        # Check if pose resembles raising pose\n",
        "\n",
        "        # Draw text above bounding box with background color\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.savefig(image_path)  # Specify the file name and extension\n",
        "        plt.imshow(image_rgb)\n",
        "\n",
        "        mp_drawing = mp.solutions.drawing_utils\n",
        "        mp_drawing.draw_landmarks(image_rgb, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
        "\n",
        "        # Draw connections between all specified landmarks\n",
        "        landmarks = results.pose_landmarks.landmark\n",
        "        connections = [(mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER),\n",
        "                    (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_HIP),\n",
        "                    (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_HIP),\n",
        "                    (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP),\n",
        "                    (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_WRIST),\n",
        "                    (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_WRIST),\n",
        "                    (mp_pose.PoseLandmark.LEFT_WRIST, mp_pose.PoseLandmark.LEFT_INDEX),\n",
        "                    (mp_pose.PoseLandmark.RIGHT_WRIST, mp_pose.PoseLandmark.RIGHT_INDEX)]\n",
        "\n",
        "        # Draw points for knees, hips, and shoulders\n",
        "        for landmark in [mp_pose.PoseLandmark.LEFT_INDEX, mp_pose.PoseLandmark.RIGHT_INDEX,\n",
        "                        mp_pose.PoseLandmark.LEFT_WRIST, mp_pose.PoseLandmark.RIGHT_WRIST,\n",
        "                            mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP,\n",
        "                            mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER]:\n",
        "                plt.scatter(results.pose_landmarks.landmark[landmark.value].x * image.shape[1],\n",
        "                            results.pose_landmarks.landmark[landmark.value].y * image.shape[0],\n",
        "                            color='white', s=100)\n",
        "\n",
        "        for connection in connections:\n",
        "            start_point = connection[0]\n",
        "            end_point = connection[1]\n",
        "            plt.plot([landmarks[start_point.value].x * image.shape[1], landmarks[end_point.value].x * image.shape[1]],\n",
        "                    [landmarks[start_point.value].y * image.shape[0], landmarks[end_point.value].y * image.shape[0]],\n",
        "                    color='magenta', linestyle='-', linewidth=1)\n",
        "        # Draw landmarks and connections\n",
        "        mp_drawing = mp.solutions.drawing_utils\n",
        "        mp_drawing.draw_landmarks(image_rgb, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
        "                                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=5),\n",
        "                                connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2))\n",
        "\n",
        "        plt.axis('off')\n",
        "        # Save the plot as bytes in memory\n",
        "        buffer = io.BytesIO()\n",
        "        plt.savefig(buffer, format='png')\n",
        "        buffer.seek(0)\n",
        "        # Convert the plot bytes to base64 string\n",
        "        plot_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
        "\n",
        "        plt.close()  # Close the plot to free memory\n",
        "\n",
        "        return plot_base64, error, is_raising\n",
        "\n",
        "    # Process the image and get the base64 encoded plot\n",
        "    plot_base64, error, is_raising = process_image(image_path)\n",
        "    error_text = \"{:.2f}\".format(error)\n",
        "\n",
        "\n",
        "    if is_raising and error <= 20:\n",
        "        return jsonify({'image_base64': plot_base64, 'is_pose': True})\n",
        "    else:\n",
        "        return jsonify({'image_base64': plot_base64, 'is_pose': False})\n",
        "\n",
        "\n",
        "@app.route('/Sujud', methods=['POST', 'GET'])\n",
        "def Detect_Sujud():\n",
        "    imagefile = request.files['imagefile']\n",
        "    image_path = imagefile.filename\n",
        "    imagefile.save(image_path)\n",
        "\n",
        "    results = model.predict(image_path)\n",
        "\n",
        "    for r in results:\n",
        "        # Extract bounding box coordinates\n",
        "        bounding_box = x1, y1, x2, y2 = r.boxes.xyxy[0].tolist()\n",
        "        print(\"Bounding box coordinates (x1, y1, x2, y2):\", x1, y1, x2, y2)\n",
        "        bounding_box_tuple = tuple(bounding_box)\n",
        "\n",
        "    bounding_box_int = tuple(int(value) for value in bounding_box_tuple)\n",
        "    print('The predicted class is ' + str(bounding_box_int))\n",
        "\n",
        "    # Initialize Mediapipe Pose model\n",
        "    mp_pose = mp.solutions.pose\n",
        "    pose = mp_pose.Pose()\n",
        "\n",
        "    # Function to calculate angle between lines formed by three points\n",
        "    def calculate_angle(a, b, c):\n",
        "        angle_rad1 = math.atan2(c[1]-b[1], c[0]-b[0]) - math.atan2(a[1]-b[1], a[0]-b[0])\n",
        "        angle_rad1 = angle_rad1 % (2 * math.pi)\n",
        "        angle_rad2 = math.atan2(a[1]-b[1], a[0]-b[0]) - math.atan2(c[1]-b[1], c[0]-b[0])\n",
        "        angle_rad2 = angle_rad2 % (2 * math.pi)\n",
        "        return math.degrees(min(angle_rad1, angle_rad2))\n",
        "\n",
        "    # Function to check if the pose resembles the Sujud position in Salat\n",
        "    def is_sujud_pose(landmarks):\n",
        "        left_knee = landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value]\n",
        "        right_knee = landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value]\n",
        "        left_ankle = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value]\n",
        "        right_ankle = landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value]\n",
        "        left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
        "        right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
        "        nose = landmarks[mp_pose.PoseLandmark.NOSE.value]\n",
        "        left_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
        "        right_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\n",
        "\n",
        "        # 1. Knees should be below hips (indicating kneeling)\n",
        "        if left_knee.y > left_hip.y and right_knee.y > right_hip.y:\n",
        "            # 2. Check if wrists are close to the nose's vertical position\n",
        "            if abs(left_wrist.y - nose.y) < 0.1 and abs(right_wrist.y - nose.y) < 0.1:\n",
        "                # 3. Nose should be lower than hips (indicating the head is close to the ground)\n",
        "                if nose.y > left_hip.y and nose.y > right_hip.y:\n",
        "                    return True, abs(left_wrist.y - nose.y) * 2 * 100\n",
        "        return False, 0\n",
        "\n",
        "    # Function to process image and detect Sujud pose\n",
        "    def process_image(image_path):\n",
        "        model_results = model.predict(image_path)\n",
        "\n",
        "        for r in model_results:\n",
        "            # Extract bounding box coordinates\n",
        "            bounding_box = x1, y1, x2, y2 = r.boxes.xyxy[0].tolist()\n",
        "            bounding_box_tuple = tuple(bounding_box)\n",
        "\n",
        "        bounding_box = tuple(int(value) for value in bounding_box_tuple)\n",
        "\n",
        "        # Load image\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Convert image to RGB\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Detect pose landmarks\n",
        "        results = pose.process(image_rgb)\n",
        "\n",
        "        # Check if pose resembles Sujud pose\n",
        "        is_sujud, error = is_sujud_pose(results.pose_landmarks.landmark)\n",
        "\n",
        "        # Red color if error>15, else green\n",
        "        if is_sujud and error <= 15:\n",
        "            color = (0, 255, 0)\n",
        "        else:\n",
        "            color = (255, 0, 0)\n",
        "\n",
        "        # Line thickness of 3 px\n",
        "        thickness = 3\n",
        "\n",
        "        # Draw bounding box with the determined color\n",
        "        x1, y1, x2, y2 = bounding_box\n",
        "        cv2.rectangle(image_rgb, (x1, y1), (x2, y2), color, thickness)\n",
        "\n",
        "\n",
        "        # Convert image to RGB\n",
        "\n",
        "        Perfect_Threshold = 5;\n",
        "\n",
        "        # Draw landmarks and connections\n",
        "        mp_drawing = mp.solutions.drawing_utils\n",
        "        mp_drawing.draw_landmarks(image_rgb, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
        "                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=5),\n",
        "                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2))\n",
        "\n",
        "        buffer = io.BytesIO()\n",
        "        plt.imshow(image_rgb)\n",
        "        plt.axis('off')\n",
        "        plt.savefig(buffer, format='png')\n",
        "        buffer.seek(0)\n",
        "        image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
        "        plt.close()\n",
        "        return image_base64, error, is_sujud\n",
        "\n",
        "    # Process the image and get the base64 encoded plot\n",
        "    plot_base64, error, is_sujud = process_image(image_path)\n",
        "    error_text = \"{:.2f}\".format(error)\n",
        "\n",
        "    if is_sujud and error <= 20:\n",
        "        return jsonify({'image_base64': plot_base64, 'is_pose': True})\n",
        "    else:\n",
        "        return jsonify({'image_base64': plot_base64, 'is_pose': False})\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(port=5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eiCuR4x4vsg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRUMpymPKq0n"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}